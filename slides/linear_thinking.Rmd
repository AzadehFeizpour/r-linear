---
title: "Linear thinking"
output: 
    ioslides_presentation:
        widescreen: true
        smaller: true
        css: style.css
        incremental: true
---

```{r include=F}
knitr::opts_chunk$set(fig.width=5, fig.height=5)
```

## Linear models

Learn to predict a response variable as

* a straight line relationship with a predictor variable
* or more than one predictor variable
* actually it doesn't have to be a straight line
* some of the predictors could be categorical
* and there could be interactions

Test

* which variables and interactions the data supports as predictors
* what confidence interval to give on their strength as predictors


## Linear models

"linear model" "general linear model" "linear predictor" "regression" "multiple regression" "multiple linear regression" "multiple linear regression model" ...

Many common statistical methods can be seen as the use of linear models in a particular way.

Many advanced statistical methods use linear models as a starting point.

> * Will finish today by looking at limma, used for RNA-Seq, microarrays, mass spectrometry, etc.



## Notation

Will give everything today in English, in maths, and in R code.

We will be using vectors and matrices.

In maths, we usually treat a vector as a matrix with a single column. In R, they are two different types. A matrix can be created using `matrix`, or `cbind` or `rbind`.

<div style="font-size: 60%">(R also makes a distinction between `matrix` and `data.frame` types. There is a good chance you have used data frames but not matrices before now. Matrices contain all the same type of value, typically numeric, whereas data frames can have different types in different columns.)</div>

<br>
Matrix transpose exchanges rows and columns. In maths it is indicated with a small t, eg $a^\top$. In R use the `t` function, eg `t(a)`.

$$
a = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} 
\quad \quad a^\top = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} 
$$

## Notation

Taking the **dot product** of two vectors we multiply corresponding elements together and add the results to obtain a total.

In maths, $a^\top b = \sum a_i b_i$.

In R:

```{r eval=F}
sum(a*b)
```

Taking the product of a matrix $X$ and a vector $a$, $Xa$, the result is a vector containing the dot product of each row of the matrix $X$ with the vector $a$.

In R:

```{r eval=F}
as.vector( X %*% a )
```

Without the `as.vector` the result would be a single column matrix.

(It's also possible to multiply two matrices with `%*%` but we won't need this today.)

## Geometry

### Distances

A vector can be thought of as a point in space. The dot product of a vector with itself $a^\top a$ is the square of its distance from the origin. Remember Pythagorus!

### Subspaces

Think of all the vectors that could result from multiplying a matrix $X$ with some arbitrary vector. Suppose the matrix $X$ has $n$ rows and $p$ columns. We obtain an (at most) $p$-dimensional **subspace** within an $n$-dimensional space.

Familiar examples of subspaces:

> * A line in two or more dimensions, passing through the origin.
> * A plane in three or more dimensions, passing through the origin.
> * A point at the origin.

In each case there is also an **orthogonal subspace** with $n-p$ dimensions.


##

Do section: Vectors and matrices


## Linear model

We imagine a **response** $y$ is produced based on $p$ **predictors** $x_j$ plus noise $\varepsilon$ ("epsilon"):

$$ y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon $$

The model has $p$ **terms**, plus a noise term. The model is specified by the choice of **coefficients** $\beta$ ("beta").

This can also be written as a dot product:

$$ y = \beta^\top x + \varepsilon $$

The noise is assumed to be normally distributed with standard deviation $\sigma$ (ie variance $\sigma^2$):

$$ \varepsilon \sim \mathcal{N}(0,\sigma^2) $$

Typically but not always $x_1$ will always be 1, so $\beta_1$ is a constant term in the model. We still count it as one of the $p$ predictors. (This matches what R actually does, but may differ from other presentations!)


## Linear model in R code

For vector of coefficients `beta` and vector of predictors in some particular case `x`, the most probable outcome is:

```{r eval=F}
y_predicted <- sum(beta*x)
```

A simulated possible outcome can be generated by adding random noise:

```{r eval=F}
y_simulated <- sum(beta*x) + rnorm(1, mean=0, sd=sigma)
```

But where do the coefficients $\beta$ come from?


## Model fitting -- estimating $\beta$

Say we have a observed $n$ responses $y_i$ with corresponding vectors of predictors $x_i$: 

$$ y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \dots + \beta_p x_{i,p} + \varepsilon_i $$

This is conveniently written in terms of a vector of responses $y$ and matrix of predictors $X$:

$$ y = X \beta + \varepsilon $$

Each response is assumed to contain the same amount of noise:

$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

## Model fitting -- estimating $\beta$ with geometry

$$ y = X \beta + \varepsilon $$
$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

A very nice consequence of assuming normal distributions is that the vector $\varepsilon$ has a spherically symmetric distribution.

The choice of $\beta$ making $y$ most likely is the one that places $X \beta$ nearest to $y$ (**Maximum Likelihood**).

Distance is the square root of the sum of squared differences in each dimension, so this is also called a **least squares** estimate. We choose $\hat \beta$ to minimize $\hat \varepsilon^\top \hat \varepsilon$.

$$ \hat \varepsilon = y - X \hat \beta $$


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
Imagine an experiment in which two noisy measurements of something are made.

Imagine many runs of this experiment.

The runs form a fuzzy circular cloud around the (noise-free) truth.
</div>

```{r echo=F}
source("../diagram.R")
begin()
many_exp()
the_truth()
```

## Model fitting -- estimating $\beta$ with geometry

```{r echo=F}
begin()
one_exp()
```


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit <- lm(y ~ 1)

coef(fit)
predict(fit)
residuals(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
```

## Model fitting -- estimating $\sigma$

<div style="float: right; width: 15em;">
The coefficient is wrong, but is our best estimate. It is an unbiassed estimate.

The residuals vector must be orthogonal to the subspace of possible predictions so it is too short, but we can correct for this to get an unbiassed estimate of the variance.

$$ \hat \sigma^2 = { \hat\varepsilon^\top \hat\varepsilon \over n-p } $$

```{r}
df.residual(fit)  # n-p
sigma(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
the_truth()
```


## 

Do section: Fitting linear models


## Formulas

Formula syntax 





## Units of observation

"individual" "biological sample"


## Predictors

"predictor" "independent variable" "explanatory variable"

Predictors can be:

* a constant (usually 1)
* something we can measure about each unit
* some varying treatment we made to each unit
* any mathematical manipulation or combination of the above that we care to dream up


## Response variable

Independent

* No *percolation* through a network, eg epidemics

* No non-random correlation, except as accounted for by predictors

* No technical replicates

Identical normal distribution of "residual" noise. Some potential problems:

* Magnitude of residual noise correlated with predictors ("heteroscedastic")

* Large outliers

* Just a few discrete values

(We can survive some lack of normality with sufficient observations. The normality of the coefficient estimates is what matters, and the Central Limit Theorem lends a hand here.)




## Noise assumptions to check

Can survive some deviation from Gaussian distribution. Central Limit Theorem means errors in estimated coefficients tend to Gaussian.

Data consisting of only a few discrete values can be a problem.

Noise should be uniform, eg across different groups, "homoscedastic".

Noise should not contain large outliers.

* The variance or even the mean might not even exist to be estimated, and then the Central Limit Theorem will not apply.

* Don't cause a Global Financial Crisis! See [Nassim Taleb](http://www.fooledbyrandomness.com/).

Various possible solutions. The first thing to try is transformation, eg log(y).


## Predictor problems

Suppose treatmeant A was applied in batch 1 and treatment B was applied only in batch 2. Treatment and batch are confounded, and a linear model can not be fit.

Assumption of uniform residual variance is more of a problem in unbalanced designs.



## Hypothesis testing


## Geometry of a hypothesis test


## Rejecting a single hypothesis is severely lacking in ambition

* If we show an effect size is non-zero, we still don't know if it is large enought to be important.

* Scientific knowledge is hypotheses that could have been rejected but have not been (Popper). The failure of a vigorous attempt to reject a hypothesis corroborates it, but how much vigorousness have we achieved?






