---
title: "Linear thinking"
author: "Paul Harrison @paulfharrison"
output: 
    ioslides_presentation:
        widescreen: true
        smaller: true
        css: style.css
        incremental: true
---

```{r include=F}
knitr::opts_chunk$set(fig.width=5, fig.height=5)
```

## Linear models

Learn to predict a response variable as

* a straight line relationship with a predictor variable
* or more than one predictor variable
* actually it doesn't have to be a straight line
* some of the predictors could be categorical
* and there could be interactions

<br/>

* test which variables and interactions the data supports as predictors
* test what confidence interval to give on their strength as predictors


## Linear models in context

"linear model" "general linear model" "linear predictor" "regression" "multiple regression" "multiple linear regression" "multiple linear regression model" ...

Linear modelling encompasses much of statistics

>* t test
>* correlation test
>* ANOVA

and is the starting point for most of the rest.

More on this at the end.


## Linear models in R

<div style="float: right">
```{r echo=F,out.width="50%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

S language (predecessor to R) was revamped in

...

substantially to support linear models:

* `data.frame` type introduced to hold data for modelling.

* `factor` type introduced to hold categorical data.

* `~` formulas specify terms in models.

* Manipulation of "S3" objects holding fitted models.

Primary reference is "Statistical models in S".


## Notation

Will give everything today in English, in maths, and in R code.

We will be using vectors and matrices.

In maths, we usually treat a vector as a matrix with a single column. In R, they are two different types. A matrix can be created using `matrix`, or `cbind` or `rbind`.

<div style="font-size: 60%">(R also makes a distinction between `matrix` and `data.frame` types. There is a good chance you have used data frames but not matrices before now. Matrices contain all the same type of value, typically numeric, whereas data frames can have different types in different columns.)</div>

<br>
Matrix transpose exchanges rows and columns. In maths it is indicated with a small t, eg $a^\top$. In R use the `t` function, eg `t(a)`.

$$
a = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} 
\quad \quad a^\top = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} 
$$

## Notation

Taking the **dot product** of two vectors we multiply corresponding elements together and add the results to obtain a total.

In maths, $a^\top b = \sum a_i b_i$.

In R:

```{r eval=F}
sum(a*b)
```

Taking the product of a matrix $X$ and a vector $a$, $Xa$, the result is a vector containing the dot product of each row of the matrix $X$ with the vector $a$.

In R:

```{r eval=F}
as.vector( X %*% a )
```

Without the `as.vector` the result would be a single column matrix.

(It's also possible to multiply two matrices with `%*%` but we won't need this today.)


## Geometry

### Distances

A vector can be thought of as a point in space. The dot product of a vector with itself $a^\top a$ is the square of its distance from the origin. Remember Pythagorus!

### Subspaces

Think of all the vectors that could result from multiplying a matrix $X$ with some arbitrary vector. Suppose the matrix $X$ has $n$ rows and $p$ columns. We obtain an (at most) $p$-dimensional **subspace** within an $n$-dimensional space.

Familiar examples of subspaces:

> * A line in two or more dimensions, passing through the origin.
> * A plane in three or more dimensions, passing through the origin.
> * A point at the origin.

In each case there is also an **orthogonal subspace** with $n-p$ dimensions.


##

Do section: Vectors and matrices


## Models

A **model** can be used to predict a **response** variable based on a set of **predictor** variables.

The prediction will usually be imperfect, due to random noise.

<br>

(Alternative terms: depedent variable, independent variables.)

(Using causal language, but really only modelling an association.)



## Linear model

A **response** $y$ is produced based on $p$ **predictors** $x_j$ plus noise $\varepsilon$ ("epsilon"):

$$ y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon $$

The model has $p$ **terms**, plus a noise term. The model is specified by the choice of **coefficients** $\beta$ ("beta").

This can also be written as a dot product:

$$ y = \beta^\top x + \varepsilon $$

The noise is assumed to be normally distributed with standard deviation $\sigma$ (ie variance $\sigma^2$):

$$ \varepsilon \sim \mathcal{N}(0,\sigma^2) $$

Typically but not always $x_1$ will always be 1, so $\beta_1$ is a constant term in the model. We still count it as one of the $p$ predictors. (This matches what R does, but may differ from other presentations!)


## Linear model in R code

For vector of coefficients `beta` and vector of predictors in some particular case `x`, the most probable outcome is:

```{r eval=F}
y_predicted <- sum(beta*x)
```

A simulated possible outcome can be generated by adding random noise:

```{r eval=F}
y_simulated <- sum(beta*x) + rnorm(1, mean=0, sd=sigma)
```

But where do the coefficients $\beta$ come from?


## Model fitting -- estimating $\beta$

Say we have a observed $n$ responses $y_i$ with corresponding vectors of predictors $x_i$: 

$$ y_i = \beta_1 x_{i,1} + \beta_2 x_{i,2} + \dots + \beta_p x_{i,p} + \varepsilon_i $$

This is conveniently written in terms of a vector of responses $y$ and matrix of predictors $X$:

$$ y = X \beta + \varepsilon $$

Each response is assumed to contain the same amount of noise:

$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

## Model fitting -- estimating $\beta$ with geometry

$$ y = X \beta + \varepsilon $$
$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

A very nice consequence of assuming normal distributions is that the vector $\varepsilon$ has a spherically symmetric distribution.

The choice of $\beta$ making $y$ most likely is the one that places $X \beta$ nearest to $y$ (**Maximum Likelihood**).

Distance is the square root of the sum of squared differences in each dimension, so this is also called a **least squares** estimate. We choose $\hat \beta$ to minimize $\hat \varepsilon^\top \hat \varepsilon$.

$$ \hat \varepsilon = y - X \hat \beta $$


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
Imagine an experiment in which two noisy measurements of something are made.

Imagine many runs of this experiment.

The runs form a fuzzy circular cloud around the (noise-free) truth.
</div>

```{r echo=F}
source("../diagram.R")
begin()
many_exp()
the_truth()
```

## Model fitting -- estimating $\beta$ with geometry

```{r echo=F}
begin()
one_exp()
```


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit <- lm(y ~ 1)

coef(fit)
predict(fit)
residuals(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
```

## Model fitting -- estimating $\sigma$

<div style="float: right; width: 15em;">
The coefficient is wrong, but is our best estimate. It is an unbiassed estimate.

The residuals vector must be orthogonal to the subspace of possible predictions so it is too short, but we can correct for this to get an unbiassed estimate of the variance.

$$ \hat \sigma^2 = { \hat\varepsilon^\top \hat\varepsilon \over n-p } $$

```{r}
df.residual(fit)  # n-p
sigma(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
the_truth()
```


## 

Do section: Single numerical predictor

Do section: Single factor predictor, two levels


## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)
```

Two model formulas representing two hypotheses H0 and H1.

H0 must *nest* within H1.

$\varepsilon_0$ can't be shorter than $\varepsilon_1$

Is it *surprisingly longer*? Then we reject H0.
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)

anova(fit0, fit1)
```

Here we can not reject the null hypothesis (F(1,1)=16, p=0.156).
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses -- the gory details

<div style="float: right; width: 15em">
```{r}
df0 <- df.residual(fit0)       # n-p0
RSS0 <- sum(residuals(fit0)^2)

df1 <- df.residual(fit1)       # n-p1
RSS1 <- sum(residuals(fit1)^2)

F <- (RSS0-RSS1)/(df0-df1) / (RSS1/df1) 
F
pf(F, df0-df1, df1, lower.tail=FALSE)
```
</div>
$$
RSS_0 = \hat\varepsilon_0^\top \hat\varepsilon_0 \\
RSS_1 = \hat\varepsilon_1^\top \hat\varepsilon_1
$$

<br>

$$
F = {{RSS_0-RSS_1 \over p_1-p_0} \over {RSS_1 \over n-p_1} }
$$

RSS = Residual Sum of Squares

Is the increase in RSS in H0 merely proportionate 
<br>to the extra degree(s) of freedom?

<div style="font-size: 150%">`anova(fit0, fit1)`<br>handles all this for us.</div>



## Contrasts

##

Do section: Multiple factors, many levels



## Multiple testing

Suppose some set of hypotheses are true. What chance do we run of rejecting one or more of them?

### Hypotheses are mutually exclusive

> * No problem!

Example: reject all values outside a confidence interval

### Family-Wise Error Rate (FWER) control

"*Any* false rejection will ruin my argument."

> * Tukey's Honestly Significanct Differences
> * Bonferroni correction (etc)

### False Discovery Rate (FDR) control

"I can tolerate a certain *rate* of false rejections."

> * Benjamini and Hochberg correction (etc)


## Formulas

"Wilikins-Rogers notation"

Formulas with `~` specify how to construct the model matrix $X$.

Merely a convenience, if you already have a model matrix `X` can always just use `~0+X`.

Let's discuss this in more detail.

## Data types

`numeric` vector is represented as itself.

`logical` vector is converted to 0s and 1s.

`factor` is represented as $n_\text{levels}-1$ predictors, "one-hot" representation.*

`matrix` is represented as multiple columns.

<br>

<div style="font-size: 50%">\* Encoding can be adjusted using `C( )` function. Default differs between S and R languages!</div>


## Function calls

Can calculate with function calls, or enclose maths in `I( )`.

Handy functions such as `poly` and `splines::ns` produce matrices for fitting curves.

```{r}
x <- 1:6
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ x + I(x^2) + log2(x))
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ poly(x,3))
```
</div>


## Intercept

Intercept term `1` is implicit.

Omit with `-1` or `0`.

R tries to be clever about factor encoding if intercept omitted.

```{r}
f <- factor(c("a","a","b","b","c","c"))
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ f)
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ 0 + f)
```
</div>


## Interactions

`a:b` specifies an interaction between `a` and `b`. 

All pairings of predictors from the two terms are multiplied together.

For logical and factor vectors, this produces the logical "and" of each pairing.
<br>&nbsp;

<br>

```{r}
model.matrix(~ f + x + f:x)
```

## Interactions

`a*b` is shorthand to also include main effects `a + b + a:b`.

(More obscurely `a/b` is shorthand for `a + a:b`, if `b` only makes sense in the context of each specific `a`.)

<span style="display: inline-block; width: 6em">`(a+b+c)^2`</span> is shorthand for `a + b + c + a:b + a:c + b:c`.
<br>
<span style="display: inline-block; width: 6em">`a*b*c`</span> is shorthand for `a + b + c + a:b + a:c + b:c + a:b:c`.

<br>

```{r}
model.matrix(~ f*x)
```


##

Do section: Gene expression example



## Summary

Should now be able to

* fit linear models
* understand the importance of different parts of the model
    * coefficients
    * residual standard deviation
    * residual degrees of freedom
* predict using a model
* test hypothesis using two nested linear models
* test hypotheses and calculate CIs using contrasts
* test many hypotheses using limma
* understand multiple testing correction


## Summary - problems with linear models

Incorrect transformation of y can introduce spurious interactions.

Highly correlated predictors, or completely confounded experimental design.

Problems with residual noise:

>* Non-uniform noise level between conditions
>* Remaining structure
>* Non-normal distribution (can survive some violation of this, however large outliers are bad)

> Various more advanced methods exist if these are present, such as robust model fitting and "sandwich" estimation of the accuracy of coefficients and contrasts.



## Further reading

<div style="float: right">
```{r echo=F,out.width="50%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

Primary reference is:

[Chambers](http://statweb.stanford.edu/~jmc4/) and [Hastie](http://web.stanford.edu/~hastie/) (1991) "Statistical Models in S"

<br>
Also good:

[Faraway](http://www.maths.bath.ac.uk/~jjf23/) (2014) "Linear Models with R"







